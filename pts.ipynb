{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6e2cea",
   "metadata": {},
   "source": [
    "## Understanding LLMs and Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7dd871",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f950781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 12:14:24.269648: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-11 12:14:24.269686: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-11 12:14:24.270991: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-11 12:14:24.278520: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-11 12:14:25.506817: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForMaskedLM,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9160b",
   "metadata": {},
   "source": [
    "#### Understanding Masked LM's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0fa164",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The first model we will look at is BERT, which is trained with masked tokens. As an example,\n",
    "## the text below masks the word \"box\" from a well-known movie quote.\n",
    "\n",
    "text = \"Life is like a [MASK] of chocolates.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0f169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## We'll now see how BERT is able to predict the missing word. We can use HuggingFace to load\n",
    "## a copy of the pretrained model and tokenizer.\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ace8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "152766f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[ 101, 2166, 2003, 2066, 1037,  103, 1997, 7967, 2015, 1012,  102]])\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "## Next, we'll feed our example text into the tokenizer.\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print('input_ids:', encoded_input['input_ids'])\n",
    "print('attention_mask:', encoded_input['attention_mask'])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c34554a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chocolate\n"
     ]
    }
   ],
   "source": [
    "## input_ids represents the tokenized output. Each integer can be mapped back to the corresponding string.\n",
    "\n",
    "print(tokenizer.decode([7967]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8065a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The model will then receive the output of the tokenizer. We can look at the BERT model to see exactly how\n",
    "## it was constructed and what the outputs will be like.\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f18b53",
   "metadata": {},
   "source": [
    "The model starts with an embedding of each of the 30,522 possible tokens into 768 dimensions, which at this point is simply a representation of each token without any additional information about their relationships to one another in the text. Then the encoder attention blocks are applied, updating the embeddings such that they now encode each token's contribution to the chunk of text and interactions with other tokens. Notably, this includes the masked tokens as well. The final stage is the language model head, which takes the embeddings from the masked positions back to 30,522 dimensions. Each index of this final vector corresponds to the probability that the token in that position would be the correct choice to fill the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ad2ce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 30522])\n"
     ]
    }
   ],
   "source": [
    "model_output = model(**encoded_input)\n",
    "output = model_output[\"logits\"]\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec757c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522])\n"
     ]
    }
   ],
   "source": [
    "tokens = encoded_input['input_ids'][0].tolist()\n",
    "masked_index = tokens.index(tokenizer.mask_token_id)\n",
    "logits = output[0, masked_index, :]\n",
    "\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ba08728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions: box bag bowl jar cup\n",
      "tensor([0.1764, 0.1688, 0.0419, 0.0336, 0.0262], grad_fn=<TopkBackward0>)\n"
     ]
    }
   ],
   "source": [
    "probs = logits.softmax(dim=-1)\n",
    "values, predictions = probs.topk(5)\n",
    "sequence = tokenizer.decode(predictions)\n",
    "\n",
    "print('Top 5 predictions:', sequence)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438055b",
   "metadata": {},
   "source": [
    "Printing the top 5 predictions and their respective scores, we see that BERT accurately chooses \"box\" as the most likely replacement for the mask token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cad139",
   "metadata": {},
   "source": [
    "#### Understanding Causal LM's\n",
    "\n",
    "We now repeat a similar exercise with the causal LLM GPT-2. This model generates text following an input, instead of replacing a mask within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5b96487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5241ae642ba3448cba7bfb1f608dacec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2bee8db8934770ac4dc6479e15ac98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701ba52172254e32868cb1f801b34b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0403fb505fd4e6891df210ccb75b7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4844e12589b645ecb45140b11aea5893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476e09ba1f9f42b5ba44d151802ca2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40278a08e4864f448e9bf9ba5e1a5644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb2be092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53db545",
   "metadata": {},
   "source": [
    "The model begins by embedding each of the 50,257 tokens into a 768-dimensional space, initially representing only individual token identities.Next, 12 transformer blocks apply self-attention and MLP layers, allowing tokens to interact and refine their contextual representations. These enriched embeddings capture dependencies across the sequence, enabling the model to understand language structure.  \n",
    "Finally, a linear head maps each embedding back to 50,257 logits, representing the probability distribution over the vocabulary for next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2c06612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[10462, 27428,   379,   262, 10481,   318]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We'll use a different text example, since this model works by producing tokens sequentially\n",
    "## rather than filling a mask.\n",
    "\n",
    "text = \"Swimming at the beach is\"\n",
    "model_inputs = tokenizer(text, return_tensors='pt')\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfc2e567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([257])\n"
     ]
    }
   ],
   "source": [
    "## After applying the model, the information needed to predict the next token is represented by\n",
    "## the last token. So we can access that vector by the index -1.\n",
    "\n",
    "output = model(**model_inputs)\n",
    "next_token_logits = output.logits[:, -1, :]\n",
    "next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "225e4a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[10462, 27428,   379,   262, 10481,   318,   257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "## Now add the new token to the end of the text, and feed all of it back to the model to continue\n",
    "## predicting more tokens.\n",
    "\n",
    "model_inputs['input_ids'] = torch.cat([model_inputs['input_ids'], next_token[:, None]], dim=-1)\n",
    "model_inputs[\"attention_mask\"] = torch.cat([model_inputs['attention_mask'], torch.tensor([[1]])], dim=-1)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6748c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swimming at the beach is a\n"
     ]
    }
   ],
   "source": [
    "## Here's what we have so far. The model added the word 'a' to the input text.\n",
    "\n",
    "print(tokenizer.decode(model_inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16b54f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swimming at the beach is a great\n"
     ]
    }
   ],
   "source": [
    "## Repeating all the previous steps, we then add the word 'great'.\n",
    "\n",
    "output = model(**model_inputs)\n",
    "next_token_logits = output.logits[:, -1, :]\n",
    "next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "model_inputs['input_ids'] = torch.cat([model_inputs['input_ids'], next_token[:, None]], dim=-1)\n",
    "model_inputs[\"attention_mask\"] = torch.cat([model_inputs['attention_mask'], torch.tensor([[1]])], dim=-1)\n",
    "print(tokenizer.decode(model_inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "687cd411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swimming at the beach is a great way to get a little extra energy.\n",
      "\n",
      "The beach\n"
     ]
    }
   ],
   "source": [
    "## HuggingFace automates this iterative process. We'll use the quicker approach to finish our sentence.\n",
    "\n",
    "output_generate = model.generate(**model_inputs, max_length=20, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(output_generate[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98677e",
   "metadata": {},
   "source": [
    "### Pre-training a GPT-2 model from scratch\n",
    "\n",
    "Next we'll train a GPT-2 model from scratch using English Wikipedia data. Note that we're only using a tiny subset of the data to demonstrate that the model is capable of learning. The exact same approach could be followed on the full dataset to train a more functional model, but that would require a lot of compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c312e408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5758149b3e340b8b4937c52d0a5f7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "ds_shuffle = dataset['train'].shuffle()\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_shuffle.select(range(50)),\n",
    "        \"valid\": ds_shuffle.select(range(50, 100))\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a6fb87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gymnothorax angusticeps is a moray eel found in the southeast Pacific Ocean, around Peru. It was first named by Hildebrand and Barton in 1949. It is colloquially known as the wrinkled moray.\n",
      "\n",
      "Referenc\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'][0]['text'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44280ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 2\n",
      "Input chunk lengths: [64, 17]\n",
      "Chunk mapping: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "## We'll tokenize the text, setting the context size to 128 and thus breaking each document into chunks of 128 tokens.\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"text\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8680c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5dcc174e594633857d287722723da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cda4dc81803406184ab90b76ae976ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 697\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 304\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf47965",
   "metadata": {},
   "source": [
    "Now we can set up the HuggingFace Trainer as follows. Since we're using such a small dataset, we'll need lots of epochs for the model to make progress because all of the parameters are randomly initialized at the outset. Typically, most LLM's are trained for only one epoch and more diverse examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73332843",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea492a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3ae8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"wiki-gpt2\",\n",
    "    eval_strategy=\"steps\",\n",
    "    num_train_epochs=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fe94268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8800' max='8800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8800/8800 1:07:00, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.727700</td>\n",
       "      <td>8.048859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.334700</td>\n",
       "      <td>8.357629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.654900</td>\n",
       "      <td>8.582697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.056900</td>\n",
       "      <td>8.882200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.579300</td>\n",
       "      <td>9.064018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>9.286490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>9.376274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>9.436334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>9.625267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>9.589741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>9.652143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>9.755585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>9.748881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>9.806969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>9.868187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>9.883601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>9.862160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8800, training_loss=0.5735442948341369, metrics={'train_runtime': 4021.6566, 'train_samples_per_second': 17.331, 'train_steps_per_second': 2.188, 'total_flos': 4553013657600000.0, 'train_loss': 0.5735442948341369, 'epoch': 100.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a467a780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 9.871642112731934,\n",
       " 'eval_runtime': 4.4402,\n",
       " 'eval_samples_per_second': 68.466,\n",
       " 'eval_steps_per_second': 8.558,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148ee9a",
   "metadata": {},
   "source": [
    "The training loss is low by the end, which means the model should perform very well on training examples it has seen. It does not generalize well to the validation set of course, since we deliberately overfit on a small train set.\n",
    "\n",
    "We can confirm with a couple of examples that were seen in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0748f7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David McGowan (born 27 April 1981) is an Australian high-performance ro\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(tokenized_datasets[\"train\"][0]['input_ids'][:16])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeca6898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(text, return_tensors='pt')\n",
    "print(model_inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08d34694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[11006, 11130, 45197,   357,  6286,  2681,  3035, 14745,     8,   318,\n",
       "           281,  6638,  1029,    12, 26585,   686,  5469,  3985,   290,  1966,\n",
       "          8852,  5752,   263,    13,  1081,   257,  5752,   263,   339,   373,\n",
       "           257, 13430]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_inputs['input_ids'] = model_inputs['input_ids'].to(device)\n",
    "model_inputs['attention_mask'] = model_inputs['attention_mask'].to(device)\n",
    "\n",
    "output_generate = model.generate(**model_inputs, max_new_tokens=16)\n",
    "output_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79e0f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David McGowan (born 27 April 1981) is an Australian high-performance rowing coach and former representative rower. As a rower he was a junior\n"
     ]
    }
   ],
   "source": [
    "sequence = tokenizer.decode(output_generate[0])\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f811c5ae",
   "metadata": {},
   "source": [
    "The model should do quite well at reciting text after seeing it so many times. We can be convinced that the tokenizer, model architecture, and training objective are well-suited to learning Wikipedia data. For comparison, we'll try this model on text from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "497b26b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R (Coughlan) v North and East Devon Health Authority [1999] EWCA Civ 1871 is a UK enterprise law case, concerning health care in\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(tokenized_datasets[\"valid\"][0]['input_ids'][:32])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e2eb77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R (Coughlan) v North and East Devon Health Authority [1999] EWCA Civ 1871 is a UK enterprise law case, concerning health care in the fifth Discworld. He has stated that the resource recovery industry. He is\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_inputs['input_ids'] = model_inputs['input_ids'].to(device)\n",
    "model_inputs['attention_mask'] = model_inputs['attention_mask'].to(device)\n",
    "\n",
    "output_generate = model.generate(**model_inputs, max_new_tokens=16)\n",
    "sequence = tokenizer.decode(output_generate[0])\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b824f505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R (Coughlan) v North and East Devon Health Authority [1999] EWCA Civ 1871 is a UK enterprise law case, concerning health care in the UK.\\n\\nFacts \\nMiss Coughlan claimed she should be able to remain at Mardon House, Exeter, purpose built for her and seven others with severe disabilities. After a 1971 road traffic accident, she became tetraplegic, needing constant care. Devon HA decided it should be closed in 1996, although she had been assured before it was a ‘home for life’. The Health Authority argued Mardon House had become ‘a prohibitively expensive white elephant’ which ‘left fewer resources available for other services’.\\n\\nJudgment\\nThe Court of Appeal held there was a legitimate expectation to fair treatment, with a substantive benefit of the ‘home for life’. Frustrating the expectation would be an abuse of power. However the duty to promote health, in statute, was not a duty to ensure the service was comprehensive.\\n\\nLord Woolf MR said that the failure to keep the home open was ‘equivalent to a breach of contract in private law’. He said the following.\\n\\nECHR article 8 was also discussed.\\n\\nSee also\\n\\nUnited Kingdom enterprise law\\n\\nNotes\\n\\nReferences\\n\\nUnited Kingdom enterprise case law\\nUnited Kingdom administrative case law\\nUnited Kingdom constitutional case law'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['valid'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ca8cd",
   "metadata": {},
   "source": [
    "As expected, our model is completely confused this time. We'd need to train for much longer, and on much more diverse data, before we would have a model that can sensibly complete prompts it has never seen before. This is precisely why pre-training is such an important and powerful technique. If we had to train on all of Wikipedia for every NLP application to achieve optimal performance, it would be prohibitively expensive. But there's no need to do that when we can share and reuse existing pre-trained models as we did in the first part of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6101f46a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
